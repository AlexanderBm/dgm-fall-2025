---
layout: distill
title: Lecture 03
description: Statistics, Linear Algebra, and Calculus Review
date: 2025-09-10

lecturers:
  - name: Ben Lengerich
    url: "https://adaptinfer.org"

authors:
  - name: Jeff Zhang # author's full name
    url: "#" # optional URL to the author's homepage
  - name: Youngwoo Kim
    url: "#"
  - name: Author 3
    url: "#"

editors:
  - name: Editor 1 # editor's full name
    url: "#" # optional URL to the editor's homepage

abstract: >
  This lecture provides a brief review of fundamental math skills for deep learning. It provides some basic programming techniques about tensors and PyTorch. This lecture also reviews some important math and statistics topics in deep learning, including linear algebra, probability, estimation, and linear regression.
---

## Today's Topics:
1. [Tensors in Deep Learning](#1-tensors-in-deep-learning)
2. [Tensors and PyTorch](#2-tensors-and-pytorch)
3. [Vectors, Matrices, and Broadcasting](#3-vectors-matrices-and-broadcasting)
4. [Probability Basics](#4-probability-basics)
5. [Estimation Methods](#5-estimation-methods)
6. [Linear Regression](#6-linear-regression)


## 1. Tensors in Deep Learning
- A **tensor** is a multidimensional array
- Dimensionality (order) = number of indices
- It is generalization of scalars(order-0 tensor), vectors(order-1 tensor), matrices(order-2 tensor).
- Images are common examples of tensor used as input in deep learning.
- 3D tensor : a single color image (height, width, channels)
- 4D tensor : a batch of images (batch_size, height, width, channels)


## 2. Tensors and PyTorch
- NumPy vs PyTorch: They have similar syntax but PyTorch adds:
  - GPU support,
  - automatic differentiation,
  - deep learning convenience functions.  
- Data types: mappings exist (e.g., NumPy `float64` → PyTorch `DoubleTensor`).   
- Matrix multiplication in Pytorch: `b.matmul(b)`, `b.dot(b)`, `b @ b`  .  
- Check GPU availability in PyTorch and move tensors between CPU and GPU using `b.to(torch.device('cpu'))` or `b.to(torch.device('cuda:0'))`


## 3. Vectors, Matrices, and Broadcasting
- **Notations:** 
  - x (lower case): vector
  - X (upper case): matrix
  - w: weights **(also a vector)**
  - b: bias **(a constant)**
  - z: pre-activation value
- **Vectors:**
  - A vector is an **n-by-1** matrix, where **n** is the number of **row**, and **1** is the number of **column**. 
  - In deep learning, we often represent vectors as **column vectors**.
  - The linear combination (pre-activation value) is written: $z = \mathbf{w}^\top \mathbf{x} + b$, where
  <d-math block>
  \mathbf{x} =
  \begin{bmatrix}
  x_1 \\
  x_2 \\
  \vdots \\
  x_m
  \end{bmatrix}
  \in \mathbb{R}^{m \times 1},
  \quad
  \mathbf{w} =
  \begin{bmatrix}
  w_1 \\
  w_2 \\
  \vdots \\
  w_m
  \end{bmatrix}
  \in \mathbb{R}^{m \times 1},
  \quad
  z \in \mathbb{R}
  </d-math>
- **Matrices:**
  - A matrix is an **n-by-m arrays** of numbers, where **n** is the number of **row**, and **m** is the number of **column**.
  - The linear combination is written: $\mathbf{z} = \mathbf{Xw} + b$, where

  <d-math block>
  \mathbf{X} =
  \begin{bmatrix}
  x^{[1]}_1 & x^{[1]}_2 & \cdots & x^{[1]}_m \\
  x^{[2]}_1 & x^{[2]}_2 & \cdots & x^{[2]}_m \\
  \vdots & \vdots & \ddots & \vdots \\
  x^{[n]}_1 & x^{[n]}_2 & \cdots & x^{[n]}_m
  \end{bmatrix}
  \in \mathbb{R}^{n \times m},
  \quad
  \mathbf{w} =
  \begin{bmatrix}
  w_1 \\
  w_2 \\
  \vdots \\
  w_m
  \end{bmatrix}
  \in \mathbb{R}^{m \times 1},
  \quad
  \mathbf{z} =
  \begin{bmatrix}
  z^{[1]} \\
  z^{[2]} \\
  \vdots \\
  z^{[n]}
  \end{bmatrix}
  \in \mathbb{R}^{n \times 1}
  </d-math>
  - Time Complexity of N-by-N matrices multiplication by naive algorithms: $O(n^3)$.
- **Broadcasting:** 
  - The rigorous math formula of linear combination is: $\mathbf{z} = \mathbf{Xw} + \mathbf{1}_n b$.
  - Using NumPy / PyTorch / TensorFlow, **broadcasting** helps to automatically expands the scalar $b$ into an $n \times 1$ vector.
  - Thus, in deep learning notation, we usually drop $\mathbf{1}_n$ and simply write $\mathbf{z} = \mathbf{Xw} + b$.
- **Broadcasting Usage:** 
  - Be cautious when debugging, since **broadcasting expands tensors automatically**.
  - Example:
    - `# 1 is broadcast to [1, 1, 1] to match the vector`  
    `torch.tensor([1, 2, 3]) + 1`  
    → `tensor([2, 3, 4])`

    - `# [1, 2, 3] is broadcast to [[ 1,  2,  3], [ 1, 2, 3]] to match the matrix`  
    `t = torch.tensor([[4, 5, 6], [7, 8, 9]])`  
    `t + torch.tensor([1, 2, 3])`  
    → `tensor([[ 5,  7,  9], [ 8, 10, 12]])`


## 4. Probability Basics
- **Random Variables and Distributions:**
  - **Discrete Random Variables:** Values from a **countable** set (e.g. coin flip).
    - Described by: **Probability Mass Function (PMF)**: $P(X = x)$.
    - Example: **Bernoulli Distribution**
      $P(X = x) = \theta^x (1-\theta)^{1-x}, \; x \in \{0,1\}$.
      - Example: fair coin flip ($\theta = 0.5$).
  - **Continuous Random Variables:**  Values from an interval (e.g. a height).
    - Described by **Probability Density Function (PDF)**:  $f(x)$.
    - Example: **Gaussian (Normal) Distribution**
      $f(x) = \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(x-\mu)^2}{2\sigma^2}}$
      - Called **"Normal"** because of the **Central Limit Theorem**.
      - **Standard Normal:** when $ \mu = 0, \sigma = 1 \$.
- **Central Limit Theorem (CLT):**
  - Let $X_1, X_2, \ldots, X_n$ be i.i.d. random variables with mean $\mu$ and variance $\sigma^2$.
  - Define the **sample mean**:$\bar{X}_n = \frac{1}{n}(X_1 + X_2 + \cdots + X_n)$
  - Then we have: $\frac{\bar{X}_n - \mu}{\frac{\sigma}{\sqrt{n}}} \to N(0,1)$ as $n \to \infty$
- **Joint, Marginal, and Conditional Probabilities:**
  - **Joint**: $P(A,B)$, probability of two events occurring together.
  - **Marginal**: $P(A) = \sum_B P(A, B)$, sum of joint probabilities over one variable.
  - **Conditional**: $P(A \mid B) = \frac{P(A, B)}{P(B)}$, probability of A given B.
- **Expectation:**
  - Formula:
    - Discrete: $E[X] = \sum_x x P(X = x)$ 
    - Continuous: $E[X] = \int_{-\infty}^{\infty} x f(x)\, dx$
  - Linearity:
    - $E[aX + b] = aE[X] + b$
    - $E[X_1 + X_2] = E[X_1] + E[X_2]$
  - Expectation of Functions:
    - **Discrete:**
      - $E[g(X)] = \sum_x g(x) P(X = x)$
      - Example: $X \sim \text{Bernoulli}(\theta), g(X) = X^2$, then $E[g(X)] = 1^2 \theta + 0^2 (1-\theta) = \theta$
    - **Continuous:**
      - $E[g(X)] = \int_x g(x) f(x)\, dx$
      - Example: $X \sim \text{Uniform}(0,1), \, g(X) = X^2$, then $E[g(X)] = \int_0^1 x^2 dx = \tfrac{1}{3}$
- **Variance:**
  - Formula: $Var(X) = E[(X - E[X])^2] \equiv E[X^2] - (E[X])^2$ 
  - Variance of Functions: $Var(g(X)) = E[(g(X) - E[g(X)])^2] \equiv E[X^2] - E[X]^2$
- **Covariance:**
  - $Cov(X,Y) = E[(X - E[X])(Y - E[Y])]$
  - If $X, Y$ are independent, then $Cov(X,Y) = 0$
  - $Cov(X,X) = Var(X)$
- **Correlation:**
  - $\rho(X, Y) = \dfrac{\mathrm{Cov}(X,Y)}{\sqrt{Var(X)Var(Y)}}$
    - $\rho = 1$: Perfect positive linear relationship.
    - $\rho = 0$: No linear relationship. 
    - $\rho = -1$: Perfect negative linear relationship.
    - Always in the range $[-1, 1]$.
- **Bayes'Rule:**
  - $ P(A \mid B) = \frac{P(B \mid A) P(A)}{P(B)} $
  - Example: Medical Test:
    - $P(\text{disease} \mid \text{positive test}) = \frac{P(\text{positive test} \mid \text{disease}) \, P(\text{disease})} {P(\text{positive test})} $


  



## 5. Estimation Methods
Estimation aims to **infer unknown parameters** (θ) from observed data.

#### Types of Estimation
- **Point Estimation**: Provides a single best guess of the parameter (e.g., MLE).
- **Interval Estimation**: Provides a range of plausible values, such as confidence intervals (frequentist) or credible intervals (Bayesian).

#### Maximum Likelihood Estimation (MLE)
- **Definition:**  
  \[
  \hat{\theta}_{MLE} = \arg\max_\theta L(\theta)
  \]
  where \(L(\theta) = P(\text{data}|\theta)\).

- **Interpretation:** Chooses θ that makes the observed data most "likely."
- Often optimized using the **log-likelihood**:
  \[
  \ell(\theta) = \sum_i \Big[ x_i \log \theta + (1-x_i)\log(1-\theta) \Big]
  \]
- Example: For Bernoulli trials with \(k\) successes in \(n\) trials,
  \[
  \hat{\theta}_{MLE} = \frac{k}{n}
  \]

- **Notes:**  
  - MLE does not always exist.  
  - May not be unique or admissible.  

#### Maximum A Posteriori (MAP)
- **Definition:**
  \[
  \hat{\theta}_{MAP} = \arg\max_\theta P(\theta|data) \propto P(data|\theta) P(\theta)
  \]
- Incorporates a **prior belief** \(P(\theta)\), unlike MLE.
- MAP balances **likelihood** and **prior**.

#### Regularization as MAP
- MLE with a **regularization penalty** is equivalent to MAP with a prior.
- Example:  
  - **L2 penalty (ridge regression):** corresponds to a Gaussian prior on coefficients.  
  - **L1 penalty (lasso regression):** corresponds to a Laplace prior.


## 6. Linear Regression

