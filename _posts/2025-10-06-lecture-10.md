<!doctype html>
<html>
  <head>
    <head>
  <meta charset="UTF-8" />
  <meta http-equiv="content-language" content="en" />
  <meta http-equiv="content-type" content="text/html; charset=UTF-8" />
  <meta name="viewport" content="width=device-width initial-scale=1.0" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />

  <title>
    STAT 453 | Lecture 10
  </title>
  <meta name="description" content="STAT 453 - University of Wisconsin-Madison - Fall 2025
" />

  <link
    rel="shortcut icon"
    href="/dgm-fall-2025/assets/img/favicon.ico"
  />

  <link rel="stylesheet" href="./main.css" />
  <link
    rel="canonical"
    href="/dgm-fall-2025/notes/lecture-10/"
  />

  
  <!-- Load Latex JS -->
  <script src="https://cdn.jsdelivr.net/npm/latex.js@0.11.1/dist/latex.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/latex.js@0.11.1/dist/latex.component.js"></script>
  
</head>

    <script src="/dgm-fall-2025/assets/js/distillpub/template.v2.js"></script>
    <script src="/dgm-fall-2025/assets/js/distillpub/transforms.v2.js"></script>
  </head>

  <d-front-matter>
    <script type="text/json">
      {
            "title": "Lecture 10",
            "description": "Rgularization",
            "published": "October 13, 2025",
            "lecturers": [
              
              {
                "lecturer": "Ben Lengerich",
                "lecturerURL": "https://adaptinfer.org"
              }
              
            ],
            "authors": [
              
              {
                "author": "Neev Agrawal"
              },
              
              {
                "author": "Dong Hyeon Jeon"
              },
              
              {
                "author": "Colby Kipp Beliveau"
              }
              
            ],
            "editors": [
              
              {
                "editor": ""
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script>
  </d-front-matter>

  <body>
    <header class="site-header">
  <div class="wrapper">
    <span class="site-title">
      <a class="page-link" href="https://adaptinfer.org/dgm-fall-2025/"
        >STAT 453</a
      >
    </span>

    <nav class="site-nav">
      <input type="checkbox" id="nav-trigger" class="nav-trigger" />
      <label for="nav-trigger">
        <span class="menu-icon">
          <svg viewBox="0 0 18 15" width="18px" height="15px">
            <path
              fill="#424242"
              d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"
            />
            <path
              fill="#424242"
              d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"
            />
            <path
              fill="#424242"
              d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"
            />
          </svg>
        </span>
      </label>

      <div class="trigger">
        <a class="page-link" href="/dgm-fall-2025/logistics/"
          >logistics</a
        >
        <a class="page-link" href="/dgm-fall-2025/lectures/"
          >lectures</a
        >
        <a class="page-link" href="/dgm-fall-2025/notes/">notes</a>
        <a class="page-link" href="/dgm-fall-2025/calendar/"
          >calendar</a
        >
        <a class="page-link" href="/dgm-fall-2025/homework/"
          >homework</a
        >
        <a class="page-link" href="/dgm-fall-2025/project/">project</a>
      </div>
    </nav>
  </div>
</header>

<d-article>

<h2>1. Improving Generalization</h2>
<p>Generalization refers to how well a trained model performs on unseen data.  
A model that generalizes well captures the <strong>true underlying patterns</strong> in the dataset instead of memorizing noise from the training set.</p>

<p>In machine learning, achieving high training accuracy is not enough — the ultimate goal is to ensure that the model <strong>performs well on data it has never seen before</strong>. Overfitting occurs when a model is too closely tailored to the training set, leading to poor performance on new data. Generalization techniques aim to reduce this gap.</p>

<p><strong>Key strategies to improve generalization:</strong></p>
<ul>
  <li>Collect more high-quality and diverse data</li>
  <li>Use data augmentation</li>
  <li>Reduce model capacity (simplify the model)</li>
  <li>Apply regularization (L1, L2, dropout)</li>
  <li>Use early stopping</li>
  <li>Leverage semi-supervised, self-supervised, or transfer learning approaches</li>
</ul>

<p>Each of these methods addresses overfitting in different ways — from improving the quality of data to directly constraining the model's complexity.</p>

<figure><img src="./figures/figure1.png"><figcaption>Figure 1. Strategies for improving model generalization.</figcaption></figure>

<h2>2. Data Augmentation</h2>
<p><strong>Data augmentation</strong> increases the effective dataset size by generating <em>label-preserving transformations</em>, which improves robustness and reduces overfitting.  
It’s particularly useful when labeled data is limited or expensive to obtain, such as in medical imaging or autonomous driving.</p>

<p><strong>Common augmentation methods:</strong></p>
<ul>
  <li>Random cropping and resizing</li>
  <li>Horizontal / vertical flipping</li>
  <li>Random rotation, translation, or zoom</li>
  <li>Adding noise, blur, or color jitter</li>
  <li>Mixup, CutMix, or advanced augmentation policies</li>
</ul>

<p>These augmentations simulate real-world variations and force the model to learn <strong>invariant and robust representations</strong>.  
For example, an image classifier trained with random rotations will learn to recognize objects regardless of orientation.</p>

<p>Mathematically:</p>
<p>$$\mathcal{D}_{\mathrm{aug}} = \{ (h(x_i), y_i)\;|\;(x_i, y_i)\in\mathcal{D}\}$$</p>

<figure><img src="./figures/figure2.png"><figcaption>Figure 2. Original vs. augmented data examples.</figcaption></figure>

<h2>3. Early Stopping</h2>
<p><strong>Early stopping</strong> is a simple but highly effective regularization technique.  
It stops training when validation performance stops improving — even if training accuracy continues rising.</p>

<ol>
  <li>Split the dataset into training, validation, and test sets.</li>
  <li>Monitor validation performance during training.</li>
  <li>Stop training when validation accuracy peaks (or validation loss stops decreasing).</li>
</ol>

<p>This technique relies on the observation that the model first learns general patterns before overfitting.  
By stopping training early, we capture the model at its best generalization point.</p>

<figure><img src="./figures/figure3.png"><figcaption>Figure 3. Early stopping accuracy curve.</figcaption></figure>
<figure><img src="./figures/figure4.png"><figcaption>Figure 4. Early stopping loss curve.</figcaption></figure>

<h2>4. L1 and L2 Regularization</h2>
<p><strong>Regularization</strong> discourages overly large weights and helps prevent overfitting by introducing a penalty term to the loss function.  
This encourages the model to learn simpler patterns rather than memorizing noise.</p>

<h3>4.1 L1 Regularization (Lasso)</h3>
<p>$$\mathcal{L}_{L1}=\frac{1}{n}\sum_{i=1}^nL(y^{[i]},\hat{y}^{[i]})+\frac{\lambda}{n}\sum_j|w_j|$$</p>
<ul>
  <li>Encourages <strong>sparsity</strong> (many weights become zero)</li>
  <li>Useful for feature selection in high-dimensional settings</li>
</ul>
<figure><img src="./figures/figure5.png"><figcaption>Figure 5. L1 regularization promotes sparsity.</figcaption></figure>

<h3>4.2 L2 Regularization (Ridge)</h3>
<p>$$\mathcal{L}_{L2}=\frac{1}{n}\sum_{i=1}^nL(y^{[i]},\hat{y}^{[i]})+\frac{\lambda}{n}\sum_jw_j^2$$</p>
<p>$$w_{i,j}:=w_{i,j}-\eta\left(\frac{\partial L}{\partial w_{i,j}}+\frac{2\lambda}{n}w_{i,j}\right)$$</p>
<figure><img src="./figures/figure6.png"><figcaption>Figure 6. L2 regularization produces smoother weights.</figcaption></figure>

<h2>5. Dropout</h2>
<p><strong>Dropout</strong> is one of the most widely used and powerful regularization techniques.  
It works by randomly dropping out a fraction of neurons during training. This forces the network to learn <strong>redundant, distributed representations</strong> instead of relying too heavily on specific neurons.</p>

<p><strong>Why Dropout Works:</strong></p>
<ul>
  <li>Breaks <strong>co-adaptation</strong> — neurons learn more general, independent features.</li>
  <li>Acts like an <strong>ensemble</strong> — different subnetworks are trained at each iteration.</li>
  <li>Improves <strong>robustness</strong> — no single neuron dominates the model’s predictions.</li>
</ul>

<p>Mathematically, during training each neuron output \(h_i\) is multiplied by a Bernoulli random variable \(z_i\):</p>

<p>
$$
\tilde{h}_i = z_i h_i, \quad
z_i =
\begin{cases}
0 & \text{with probability } p \\
1 & \text{with probability } 1 - p
\end{cases}
$$
</p>

<p>At inference time, dropout is disabled and weights are scaled to account for the dropped neurons during training.</p>

<figure><img src="./figures/figure7.png"><figcaption>Figure 7. Dropout network training.</figcaption></figure>
<figure><img src="./figures/figure8.png"><figcaption>Figure 8. Dropout improves validation accuracy and generalization.</figcaption></figure>

<h2>Summary</h2>
<p>Regularization and generalization techniques are essential for robust machine learning models.  
They help avoid overfitting and improve predictive performance on unseen data.  
In practice, the best models combine data augmentation, early stopping, L2 regularization, and dropout for strong generalization.</p>

</d-article>
</div>

<footer class="site-footer">
  <div class="wrapper">
    <h2 class="footer-heading">Introduction to Deep Learning and Generative Models</h2>
    <p>&copy; 2025 University of Wisconsin · STAT 453 Lecture Notes</p>
  </div>
</footer>
</body>
</html>
